{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with key/value pair RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: This notebook is worth 10% of the grade of project 2.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Introduction to Spark with Python, by Jose A. Dianes](https://github.com/jadianes/spark-py-notebooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark provides specific functions to deal with RDDs which elements are key/value pairs. They are usually used to perform aggregations and other processings by key.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will show how, by working with key/value pairs, we can process our network interactions dataset in a more practical and powerful way than that used in previous notebooks. Key/value pair aggregations will show to be particularly effective when trying to explore each type of tag in our network attacks, in an individual way.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data and creating the RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will use the reduced dataset (1 percent) provided for the KDD Cup 1999, containing nearly half million network interactions. The file is provided as a *Gzip* file in the local directory.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from pyspark import SparkContext\n",
    "data_file = \"file://\" + os.getcwd() + \"/../kddcup.data_1_percent.gz\"\n",
    "sc=SparkContext.getOrCreate() \n",
    "raw_data=sc.textFile(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a pair RDD for interaction types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we want to do some exploratory data analysis on our network interactions dataset. More concretely we want to profile each network interaction type in terms of some of its variables such as duration. In order to do so, we first need to create the RDD suitable for that, where each interaction is parsed as a CSV row representing the value, and is put together with its corresponding tag as a key.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally we create key/value pair RDDs by applying a function using `map` to the original data. This function returns the corresponding pair for a given RDD element. We can proceed as follows.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'>\n"
     ]
    }
   ],
   "source": [
    "csv_data = raw_data.map(lambda x: x.split(\",\"))\n",
    "# TODO: From each row of the data, generate a key-value pair. The key will be the tag and the value will be the CSV data.\n",
    "# HINT: x[41] contains the network interaction tag\n",
    "key_value_data = csv_data.map(lambda x: (x[41], x))\n",
    "print(type(key_value_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now our key/value pair data ready to be used. Let's get the first element in order to see how it looks like.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('normal.',\n",
       "  ['0',\n",
       "   'udp',\n",
       "   'private',\n",
       "   'SF',\n",
       "   '105',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '2',\n",
       "   '2',\n",
       "   '0.00',\n",
       "   '0.00',\n",
       "   '0.00',\n",
       "   '0.00',\n",
       "   '1.00',\n",
       "   '0.00',\n",
       "   '0.00',\n",
       "   '255',\n",
       "   '241',\n",
       "   '0.95',\n",
       "   '0.01',\n",
       "   '0.01',\n",
       "   '0.00',\n",
       "   '0.00',\n",
       "   '0.00',\n",
       "   '0.00',\n",
       "   '0.00',\n",
       "   'normal.'])]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_value_data.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data aggregations with key/value pair RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use all the transformations and actions available for normal RDDs with key/value pair RDDs. We just need to make the functions work with pair elements. Additionally, Spark provides specific functions to work with RDDs containing pair elements. They are very similar to those available for general RDDs.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we have a `reduceByKey` transformation that we can use as follows to calculate the total duration of each network interaction type.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('land.', 0.0),\n",
       " ('portsweep.', 250025.0),\n",
       " ('loadmodule.', 103.0),\n",
       " ('neptune.', 0.0),\n",
       " ('buffer_overflow.', 237.0),\n",
       " ('satan.', 12.0),\n",
       " ('pod.', 0.0),\n",
       " ('ipsweep.', 0.0),\n",
       " ('back.', 48.0),\n",
       " ('teardrop.', 0.0),\n",
       " ('nmap.', 0.0),\n",
       " ('smurf.', 0.0),\n",
       " ('warezclient.', 88429.0),\n",
       " ('guess_passwd.', 0.0),\n",
       " ('normal.', 2127905.0),\n",
       " ('warezmaster.', 19.0),\n",
       " ('imap.', 0.0)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_value_duration = csv_data.map(lambda x: (x[41], float(x[0]))) \n",
    "# TODO: Aggregate the durations of network interactions which have the same key (i.e., tag) \n",
    "# HINT: The argument of reduceByKey is a lambda which takes two values and returns the reduced result.\n",
    "durations_by_key = key_value_duration.reduceByKey(lambda x, y: x+y)\n",
    "\n",
    "durations_by_key.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a specific counting action for key/value pairs.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'back.': 195,\n",
       "             'buffer_overflow.': 3,\n",
       "             'guess_passwd.': 5,\n",
       "             'imap.': 1,\n",
       "             'ipsweep.': 134,\n",
       "             'land.': 3,\n",
       "             'loadmodule.': 1,\n",
       "             'neptune.': 10704,\n",
       "             'nmap.': 24,\n",
       "             'normal.': 9641,\n",
       "             'pod.': 28,\n",
       "             'portsweep.': 85,\n",
       "             'satan.': 161,\n",
       "             'smurf.': 28219,\n",
       "             'teardrop.': 92,\n",
       "             'warezclient.': 103,\n",
       "             'warezmaster.': 3})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: count the number of rows for each key\n",
    "counts_by_key = key_value_data.countByKey()\n",
    "\n",
    "counts_by_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `combineByKey`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the most general of the per-key aggregation functions. Most of the other per-key combiners are implemented using it. We can think about it as the `aggregate` equivalent since it allows the user to return values that are not the same type as our input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`combineByKey` is essentially a combination of `map` and `reduce`. `combineByKey` requires three lambda functions as arguments:\n",
    "  - **createCombiner**: Given a value V, return a combination of values or tuples\n",
    "  - **mergeValue**: Given a combination C and a value V, returns a combination of values or tuples\n",
    "  - **mergeCombiners**: Given two combinations C1 and C2, returns a combination of values or tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we can use it to calculate per-type average durations as follows.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'back.': (48.0, 195),\n",
       " 'buffer_overflow.': (237.0, 3),\n",
       " 'guess_passwd.': (0.0, 5),\n",
       " 'imap.': (0.0, 1),\n",
       " 'ipsweep.': (0.0, 134),\n",
       " 'land.': (0.0, 3),\n",
       " 'loadmodule.': (103.0, 1),\n",
       " 'neptune.': (0.0, 10704),\n",
       " 'nmap.': (0.0, 24),\n",
       " 'normal.': (2127905.0, 9641),\n",
       " 'pod.': (0.0, 28),\n",
       " 'portsweep.': (250025.0, 85),\n",
       " 'satan.': (12.0, 161),\n",
       " 'smurf.': (0.0, 28219),\n",
       " 'teardrop.': (0.0, 92),\n",
       " 'warezclient.': (88429.0, 103),\n",
       " 'warezmaster.': (19.0, 3)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: generate `sum_counts` as to sum up the elements of the same key, and for each key, to return a tuple of (sum, count).\n",
    "\n",
    "    \n",
    "# http://www.hadoopexam.com/adi/index.php/spark-blog/90-how-combinebykey-works-in-spark-step-by-step-explaination   \n",
    "\n",
    "sum_counts = key_value_duration.combineByKey(\n",
    "    \n",
    "    (lambda v: (v, 1)), # create a combiner http://abshinn.github.io/python/apache-spark/2014/10/11/using-combinebykey-in-apache-spark/\n",
    "    (lambda c, v: (c[0]+v, c[1]+1)), # sum value and increment count\n",
    "    (lambda c1, c2: (c1[0]+c2[0], c1[1]+c2[1])) # merge two combiners\n",
    ")\n",
    "\n",
    "sum_counts.collectAsMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the arguments are pretty similar to those passed to `aggregate` in the previous notebook. The result associated to each type is in the form of a pair. If we want to actually get the averages, we need to do the division before collecting the results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "portsweep.: 2941.470588235294\n",
      "warezclient.: 858.5339805825242\n",
      "normal.: 220.71413753759984\n",
      "loadmodule.: 103.0\n",
      "buffer_overflow.: 79.0\n",
      "warezmaster.: 6.333333333333333\n",
      "back.: 0.24615384615384617\n",
      "satan.: 0.07453416149068323\n",
      "imap.: 0.0\n",
      "nmap.: 0.0\n",
      "neptune.: 0.0\n",
      "teardrop.: 0.0\n",
      "ipsweep.: 0.0\n",
      "land.: 0.0\n",
      "pod.: 0.0\n",
      "smurf.: 0.0\n",
      "guess_passwd.: 0.0\n"
     ]
    }
   ],
   "source": [
    "# TODO: create a RDD 'duration_means_by_type' by mapping each key-value pair of `sum_counts` as the key and the duration mean\n",
    "# HINT: duration mean = sum / count\n",
    "duration_means_by_type = sum_counts.map(lambda x: (x[0], x[1][0] / x[1][1])).collectAsMap()\n",
    "#sum_counts.map(lambda key, value: (key, value[0] / value[1]))\n",
    "#lambda (label, (value_sum, count)): (label, value_sum / count)\n",
    "#duration_means_by_type=duration_means_by_type.collectAsMap()\n",
    "# Print them sorted\n",
    "#duration_means_by_type.collectAsMap()\n",
    "for tag in sorted(duration_means_by_type, key=duration_means_by_type.get, reverse=True):\n",
    "    print(tag + \": \" + str(duration_means_by_type[tag]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
